{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1Nmm2dYUPQZ8ZDcX/KOeu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiemanBall/Machine-Learning/blob/master/LogisticRegression/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xea6tgrM3H3S",
        "colab_type": "text"
      },
      "source": [
        "# Classification with Logistic Regression\n",
        "This notebook build a Logistic Regression from scratch to predict whether the income of an indivisual exceeds 50,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgfqocz7QYcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import linear_model\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpLTBiGz8QXZ",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "The dataset used here is from https://www.kaggle.com/c/ml2020spring-hw2/data, which is modified from [**Census-Income (KDD) Data Set**](https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)) and can be found in [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/ml/index.php)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ahcr5WARCIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "d1df8181-a211-4433-a7ec-ef7798cc79b2"
      },
      "source": [
        "!gdown --id '1KSFIRh0-_Vr7SdiSCZP1ItV7bXPxMD92' --output data.tar.gz\n",
        "!tar -zxvf data.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KSFIRh0-_Vr7SdiSCZP1ItV7bXPxMD92\n",
            "To: /content/data.tar.gz\n",
            "6.11MB [00:00, 16.8MB/s]\n",
            "data/\n",
            "data/sample_submission.csv\n",
            "data/test_no_label.csv\n",
            "data/train.csv\n",
            "data/X_test\n",
            "data/X_train\n",
            "data/Y_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv5QM-sxIwqv",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing\n",
        "- Read data:\n",
        "\n",
        "    > Note that `X_train` and `X_test` is already preprocessed with one-hot encoding.\n",
        "\n",
        "- Normalize data\n",
        "- Split to training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UOsJhFhIv74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read data from csv files\n",
        "X_train_fpath = './data/X_train'\n",
        "Y_train_fpath = './data/Y_train'\n",
        "X_test_fpath = './data/X_test'\n",
        "output_fpath = './output_{}.csv'\n",
        "\n",
        "# Parse csv files to numpy array\n",
        "with open(X_train_fpath) as f:\n",
        "    next(f)\n",
        "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
        "with open(Y_train_fpath) as f:\n",
        "    next(f)\n",
        "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
        "with open(X_test_fpath) as f:\n",
        "    next(f)\n",
        "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1wMmIvdWVkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(X, train=True, columns=None, X_mean=None, X_std=None):\n",
        "    '''\n",
        "    This function serves for normalizing both training set and testing set.\n",
        "    Inputs:\n",
        "        X: Nxm ndarray. N data, m features\n",
        "        train: Boolean. True when X is training data. False when X is testing data\n",
        "        columns: ndarray. Specify the indices of columns to be normalized\n",
        "        X_mean: Nx1 ndarray. N Mean values for each data point, used to normalize testing data\n",
        "        X_std: Nx1 ndarray. N standard deviation for each data point, used to normalize testing data\n",
        "    Outputs:\n",
        "        X: normalized data\n",
        "        X_mean: mean of X if X is training data\n",
        "        X_std: standard deviation of X if X is training data\n",
        "    '''\n",
        "    if columns is None:\n",
        "        columns = np.arange(X.shape[1])\n",
        "\n",
        "    if train:\n",
        "        X_mean = np.mean(X[:, columns], 0).reshape(1, -1)\n",
        "        X_std = np.std(X[:, columns], 0).reshape(1, -1)\n",
        "\n",
        "    X[:, columns] = (X[:, columns] - X_mean) / (X_std + 1e-8)   # In case X_std is zero\n",
        "\n",
        "    return X, X_mean, X_std\n",
        "\n",
        "\n",
        "# Normalize dataset\n",
        "X_train, X_mean, X_std = normalize(X_train, train = True)\n",
        "X_test, _, _= normalize(X_test, train = False, columns = None, X_mean = X_mean, X_std = X_std)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CePHoEMCn9eQ",
        "colab_type": "text"
      },
      "source": [
        "### Some helping functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmTy96KiX_4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helping function\n",
        "def sigmoid(z):\n",
        "    # To avoid overflow, minimum/maximum output value is set.\n",
        "    return np.clip(1.0 / (1.0 + np.exp(-z)), 1e-8, 1 - 1e-8)\n",
        "\n",
        "def adam(grads, vs, ms, t):\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "    eps_stable = 1e-6\n",
        "    \n",
        "    for grad, v, m in zip(grads, vs, ms):\n",
        "        m[:] = beta1 * m + (1 - beta1) * grad\n",
        "        v[:] = beta2 * v + (1 - beta2) * np.square(grad)\n",
        "\n",
        "        m_bias_corr = m / (1 - beta1 ** t)\n",
        "        v_bias_corr = v / (1 - beta2 ** t)\n",
        "\n",
        "        grad[:] = m_bias_corr / (np.sqrt(v_bias_corr) + eps_stable)\n",
        "\n",
        "    return grads, vs, ms\n",
        "\n",
        "\n",
        "def shuffle(X, Y):\n",
        "    '''\n",
        "    Shuffle dataset. X and Y should have same length at 0-dimension.\n",
        "    Inputs:\n",
        "        X: Nxm ndarray. N data, m features\n",
        "        Y: Nx1 ndarray. N data, 1 label\n",
        "    Outputs:\n",
        "        X: Nxm ndarray. Shuffled X. N data, m features\n",
        "        Y: Nx1 ndarray. Shuffled Y. N data, 1 label\n",
        "    '''\n",
        "    if X.shape[0] != Y.shape[0]:\n",
        "        print(\"Shape of X doesn't match with shape of Y\")\n",
        "        return None\n",
        "\n",
        "    if len(Y.shape) < 2:\n",
        "        Y = Y.reshape(-1, 1)\n",
        "\n",
        "    rand_indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(rand_indices)\n",
        "    return X[rand_indices, :], Y[rand_indices, :]\n",
        "\n",
        "def accuracy(Y_pred_class, Y_truth):\n",
        "    acc = 1 - np.mean(np.abs(Y_pred_class - Y_truth))\n",
        "    return acc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDhoEUHew323",
        "colab_type": "text"
      },
      "source": [
        "### Split the data\n",
        "Before splitting the data, we shuffle it to have random sequence. Then we split to training set and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uys0UoerUdh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "55866972-4069-47c7-9e3f-ada04b071c0d"
      },
      "source": [
        "def split_train_val(X, Y, train_ratio=0.8):\n",
        "    '''\n",
        "    Split dataset (X, Y) to training set and validation set with training ratio = train_ratio\n",
        "    '''\n",
        "    num_data = X.shape[0]\n",
        "    train_sz = ceil(num_data * train_ratio)\n",
        "    return X[:train_sz], Y[:train_sz], X[train_sz:], Y[train_sz:]\n",
        "\n",
        "# Shuffle the data\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "# Split to training set and validation set\n",
        "X_train_set, Y_train_set, X_val_set, Y_val_set = split_train_val(X_train, Y_train)\n",
        "\n",
        "train_size = X_train_set.shape[0]\n",
        "val_size = X_val_set.shape[0]\n",
        "test_size = X_test.shape[0]\n",
        "data_dim = X_train_set.shape[1]\n",
        "print('Size of training set: {}'.format(train_size))\n",
        "print('Size of validation set: {}'.format(val_size))\n",
        "print('Size of testing set: {}'.format(test_size))\n",
        "print('Dimension of data: {}'.format(data_dim))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: 43405\n",
            "Size of validation set: 10851\n",
            "Size of testing set: 27622\n",
            "Dimension of data: 510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6D3kL4VX4M7",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression Model\n",
        "The LR model contains:\n",
        "- Cost function: Regularized cross-entropy\n",
        "\n",
        "    For a binary classification, the cost function is:\n",
        "    \n",
        ">> L = cross entropy loss + regularization\n",
        "\n",
        ">> cross entropy loss = $-\\{y^T\\log(h_w(X)) + (1-y)^T\\log(1-h_w(X))\\}$\n",
        "\n",
        ">> regularization = \n",
        "\\begin{cases}\n",
        "    \\lambda ||W||_1  & \\text{Lasso}\\\\\n",
        "    \\frac{\\lambda}{2}\\sum^m_i{w_i^2} & \\text{L2-norm}\\\\\n",
        "\\end{cases} \n",
        "\n",
        ">> where\n",
        "\n",
        "\\begin{equation}\n",
        "X \\in R^{n\\times m}\\\\\n",
        "h_w(X) = \\sigma(XW + b)\\\\\n",
        "\\end{equation}\n",
        "\n",
        ">> with $n$ = batch size, $m$ = number of features\n",
        "\n",
        "- Gradient of cost function w.r.t. weight and bias\n",
        "\\\n",
        "\\\n",
        "\\begin{equation}\n",
        "\\frac{\\partial{L}}{\\partial{W}} = X^T (h_w(X) - y) + \n",
        "    \\begin{cases}\n",
        "        \\lambda sign(W)  & \\text{Lasso}\\\\\n",
        "        \\lambda \\cdot W & \\text{L2-norm}\\\\\n",
        "    \\end{cases}\\\\\n",
        "\\frac{\\partial{L}}{\\partial{b}} = (h_w(X) - y)\n",
        "\\end{equation}\n",
        "\n",
        "- Training function\n",
        "- Prediction function\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvfI2j1yZejL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from enum import Enum\n",
        "class Regularization(Enum):\n",
        "    L1 = 1\n",
        "    L2 = 2\n",
        "\n",
        "\n",
        "class LogisticRegression (object):\n",
        "    def __init__(self, lr = 1e-1, max_epoch = 10, reg_rate = 1e-2):\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "        self.regularization = None\n",
        "        self.train_loss = []\n",
        "        self.train_acc = []\n",
        "        self.val_loss = []\n",
        "        self.val_acc = []\n",
        "        self.loss_converge = 1e-4\n",
        "        self.acc_converge = 1e-4\n",
        "        self.patience = 3\n",
        "\n",
        "        self.learning_rate = lr\n",
        "        self.max_epoch = max_epoch\n",
        "        self.reg_rate = reg_rate\n",
        "    \n",
        "    # Cost function\n",
        "    def cross_entropy_loss(self, Y_pred, Y_truth):\n",
        "        cross_entropy = -Y_truth.T @ np.log(Y_pred) - (1 - Y_truth.T) @ np.log(1 - Y_pred)\n",
        "        return cross_entropy.item() + self.regularization_loss()\n",
        "        \n",
        "    def regularization_loss(self):\n",
        "        if self.regularization == Regularization.L1:\n",
        "            return self.reg_rate * np.linalg.norm(self.W, ord = 1)\n",
        "        elif self.regularization == Regularization.L2:\n",
        "            return self.reg_rate / 2.0 * np.sum(np.square(self.W))\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def gradient(self, X, Y_truth):\n",
        "        # Compute the gradient dLdW and dLdb\n",
        "        Y_pred = self.predict(X)\n",
        "        error = Y_pred - Y_truth\n",
        "        dLdW = X.T @ error + self.regularization_gradient()\n",
        "        dLdb = np.sum(error).reshape(1,)\n",
        "\n",
        "        return [dLdW, dLdb]\n",
        "\n",
        "    def regularization_gradient(self):\n",
        "        if self.regularization == Regularization.L1:\n",
        "            return self.reg_rate * np.sign(self.W)\n",
        "        elif self.regularization == Regularization.L2:\n",
        "            return self.reg_rate * self.W\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def train(self, X_train, Y_train, X_val = None, Y_val = None, batch_size = 10, regularization = None, verbose = False):\n",
        "        self.regularization = regularization\n",
        "        num_train, num_features = X_train.shape\n",
        "        \n",
        "        if len(Y_train.shape) < 2:\n",
        "            Y_train = Y_train.reshape(-1, 1)\n",
        "\n",
        "        if X_val is not None:\n",
        "            num_val = X_val.shape[0]\n",
        "            if len(Y_val.shape) < 2:\n",
        "                Y_val = Y_val.reshape(-1, 1)\n",
        "\n",
        "        # Xavier Initialization\n",
        "        bound = 1.0 / np.sqrt(num_train)\n",
        "        self.W = np.random.uniform(low = -bound, high = bound, size = num_features).reshape(-1, 1)\n",
        "        self.b = np.zeros((1,))\n",
        "        vs = [np.zeros_like(self.W), np.zeros_like(self.b)]\n",
        "        ms = [np.zeros_like(self.W), np.zeros_like(self.b)]\n",
        "        step = 0\n",
        "\n",
        "        best_loss = np.Inf\n",
        "        best_acc  = 0\n",
        "        loss_violate = 0\n",
        "        acc_violate  = 0\n",
        "\n",
        "        for epoch in range(self.max_epoch):\n",
        "            # Shuffle the training data\n",
        "            X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            num_batches = num_train // batch_size\n",
        "            for i_batch in range(num_batches):\n",
        "                start = i_batch * batch_size\n",
        "                end = (i_batch + 1) * batch_size if i_batch != num_batches - 1 else num_train   # Use all the rest if this is the last batch\n",
        "                X = X_train[start : end]\n",
        "                Y = Y_train[start : end]\n",
        "\n",
        "                step += 1\n",
        "\n",
        "                # dLdW, dLdb = self.gradient(X, Y)\n",
        "                grads = self.gradient(X, Y)\n",
        "                [dLdW, dLdb], vs, ms = adam(grads, vs, ms, step)\n",
        "\n",
        "                self.W -= self.learning_rate * dLdW\n",
        "                self.b -= self.learning_rate * dLdb\n",
        "\n",
        "            # Compute and record loss and accuracy\n",
        "            Y_train_pred_prob  = self.predict(X_train)\n",
        "            Y_train_pred_class = np.around(Y_train_pred_prob).astype(np.int)\n",
        "            self.train_loss.append(self.cross_entropy_loss(Y_train_pred_prob, Y_train) / num_train)\n",
        "            self.train_acc.append(accuracy(Y_train_pred_class, Y_train))\n",
        "            \n",
        "            if np.isnan(self.train_loss[-1]):\n",
        "                print(\"Training loss is NAN\")\n",
        "                return\n",
        "\n",
        "            if X_val is not None:\n",
        "                Y_val_pred_prob  = self.predict(X_val)\n",
        "                Y_val_pred_class = np.around(Y_val_pred_prob).astype(np.int)\n",
        "                self.val_loss.append(self.cross_entropy_loss(Y_val_pred_prob, Y_val) / num_val)\n",
        "                self.val_acc.append(accuracy(Y_val_pred_class, Y_val))\n",
        "            \n",
        "                if np.isnan(self.val_loss[-1]):\n",
        "                    print(\"Validation loss is NAN\")\n",
        "                    return\n",
        "\n",
        "            if verbose and (epoch % 10 == 0 or epoch == self.max_epoch - 1):\n",
        "                print(f\"Epoch: {epoch}\")\n",
        "                print(f'Training loss: {self.train_loss[-1]}')\n",
        "                if X_val is not None:\n",
        "                    print(f'Validation loss: {self.val_loss[-1]}')\n",
        "                print(f'Training accuracy: {self.train_acc[-1]}')\n",
        "                if X_val is not None:\n",
        "                    print(f'Validation accuracy: {self.val_acc[-1]}')\n",
        "                print('--------------------------------------------------------------')\n",
        "\n",
        "            # Early exit for loss convergence\n",
        "            if len(self.train_loss) > 1 and best_loss > self.train_loss[-1]:\n",
        "                best_loss = self.train_loss[-1]\n",
        "                loss_violate = 0\n",
        "            elif self.train_loss[-1] - best_loss < self.loss_converge:\n",
        "                loss_violate += 1\n",
        "                if loss_violate > self.patience:\n",
        "                    print(\"Early exit -- Loss converges!\")\n",
        "                    return\n",
        "\n",
        "            # Early exit for accuracy convergence\n",
        "            if len(self.train_acc) > 1 and best_acc < self.train_acc[-1]:\n",
        "                best_acc = self.train_acc[-1]\n",
        "                acc_violate = 0\n",
        "            elif best_acc - self.train_acc[-1] < self.acc_converge:\n",
        "                acc_violate += 1\n",
        "                if acc_violate > self.patience:\n",
        "                    print(\"Early exit -- Accuracy converges!\")\n",
        "                    return\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Output of logistic regression function.\n",
        "        Inputs:\n",
        "            X: Nxm ndarray. N data, m features\n",
        "        Outputs:\n",
        "            Probabilities: Nx1 ndarray. Predicted probability of each row of X being positively labeled\n",
        "        '''\n",
        "        return sigmoid(X @ self.W + self.b)\n",
        "\n",
        "    def classify(self, X):\n",
        "        '''\n",
        "        Classify the class by rounding the predicted probability computed from each row of features in X\n",
        "        Inputs:\n",
        "            X: Nxm ndarray. N data, m features\n",
        "        Outputs:\n",
        "            classes: Nx1 ndarray. \n",
        "        '''\n",
        "        return np.around(self.predict(X)).astype(np.int)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYWv0iBF2Mot",
        "colab_type": "text"
      },
      "source": [
        "## Train LR model\n",
        "- Use K-fold cross validation\n",
        "- We will implement grid search to find the best hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWz7S68WJXQe",
        "colab_type": "text"
      },
      "source": [
        "### K-fold cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FT0MFyprFtF",
        "colab_type": "text"
      },
      "source": [
        "#### L1 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuTBanV5q6zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(10, True, 1)\n",
        "lrs = [1e-3, 5e-4, 1e-4]\n",
        "reg_rates = [1e-3, 5e-4, 1e-4, 0]\n",
        "\n",
        "accs_l1 = [[] for _ in range(len(lrs))]\n",
        "\n",
        "for i, lr in enumerate(lrs):\n",
        "    for j, reg_r in enumerate(reg_rates):\n",
        "        print(f\"Learning rate: {lr}, regularization rate: {reg_r}\")\n",
        "\n",
        "        LR = LogisticRegression(lr = lr, max_epoch = 20, reg_rate = reg_r)\n",
        "        \n",
        "        # enumerate splits\n",
        "        for k, (train_ind, test_ind) in enumerate(kfold.split(X_train)):\n",
        "            print(f\"No.{k}-fold\")\n",
        "            X_train_set, Y_train_set = X_train[train_ind], Y_train[train_ind]\n",
        "            X_val_set, Y_val_set = X_train[test_ind], Y_train[test_ind]\n",
        "            LR.train(X_train_set, Y_train_set, X_val_set, Y_val_set, batch_size = 20, regularization = Regularization.L1, verbose = True)\n",
        "\n",
        "        accs_l1[i].append((LR.train_acc[-1], LR.val_acc[-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGJEcdNdeKSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9b2cb015-726a-4971-883c-2fe9f24a2e9c"
      },
      "source": [
        "# Based on the result of using L1 regularization, we choose lr = 5e-4 and no regularization\n",
        "for row in accs_l1:\n",
        "    for train, val in row:\n",
        "        print(f\"({train:.6f},{val:.6f})\", end=', ')\n",
        "    print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.884049,0.884424), (0.881940,0.882028), (0.883373,0.882028), (0.884745,0.880369), \n",
            "(0.884786,0.883134), (0.884807,0.882949), (0.884889,0.884608), (0.885012,0.885346), \n",
            "(0.883967,0.883502), (0.884233,0.882949), (0.883762,0.884793), (0.883762,0.882949), \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm68NagvrJ3f",
        "colab_type": "text"
      },
      "source": [
        "#### L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RWbOihv2PgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = KFold(10, True, 1)\n",
        "lrs = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4]\n",
        "reg_rates = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 0]\n",
        "\n",
        "accs_l2 = [[] for _ in range(len(lrs))]\n",
        "\n",
        "for i, lr in enumerate(lrs):\n",
        "    for j, reg_r in enumerate(reg_rates):\n",
        "        print(f\"Learning rate: {lr}, regularization rate: {reg_r}\")\n",
        "\n",
        "        LR = LogisticRegression(lr = lr, max_epoch = 20, reg_rate = reg_r)\n",
        "        \n",
        "        # enumerate splits\n",
        "        for k, (train_ind, test_ind) in enumerate(kfold.split(X_train)):\n",
        "            print(f\"No.{k}-fold\")\n",
        "            X_train_set, Y_train_set = X_train[train_ind], Y_train[train_ind]\n",
        "            X_val_set, Y_val_set = X_train[test_ind], Y_train[test_ind]\n",
        "            LR.train(X_train_set, Y_train_set, X_val_set, Y_val_set, batch_size = 20, regularization = Regularization.L2, verbose = True)\n",
        "\n",
        "        accs_l2[i].append((LR.train_acc[-1], LR.val_acc[-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re-On0nprPwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "9c9a4bee-4a0b-4453-a402-a7011806e6a8"
      },
      "source": [
        "# Based on the result of using L2 regularization, we choose lr = 5e-4 and regularization rate = 1e-3\n",
        "for row in accs_l2:\n",
        "    for train, val in row:\n",
        "        print(f\"({train:.6f},{val:.6f})\", end=', ')\n",
        "    print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.863734,0.860461), (0.863796,0.857696), (0.861830,0.858802), (0.864471,0.861198), (0.864942,0.861751), (0.865372,0.857696), \n",
            "(0.873523,0.867097), (0.874424,0.873364), (0.868035,0.867281), (0.871086,0.869309), (0.873298,0.871336), (0.871946,0.864885), \n",
            "(0.883865,0.883687), (0.883230,0.884608), (0.883271,0.880369), (0.882820,0.880184), (0.883947,0.877972), (0.883435,0.880922), \n",
            "(0.885360,0.884055), (0.885749,0.882949), (0.885360,0.884055), (0.884479,0.883871), (0.883455,0.882396), (0.884418,0.882765), \n",
            "(0.883639,0.883871), (0.883701,0.882028), (0.884172,0.883318), (0.884049,0.882949), (0.884151,0.883502), (0.883906,0.884793), \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaeYfr2ojst6",
        "colab_type": "text"
      },
      "source": [
        "### Train the model\n",
        "Based on the grid search result, we pick the hyperparameters as shown in the code cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dw6Qjj5qfSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "outputId": "11924923-f35e-4883-f4ce-717ac42c3f65"
      },
      "source": [
        "X_train_set, Y_train_set, X_val_set, Y_val_set = split_train_val(X_train, Y_train)\n",
        "\n",
        "LR = LogisticRegression(lr = 5e-4, max_epoch = 50, reg_rate = 1e-3)\n",
        "LR.train(X_train_set, Y_train_set, X_val_set, Y_val_set, batch_size = 20, regularization = Regularization.L2, verbose = True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Training loss: 0.3555621849024444\n",
            "Validation loss: 0.35663918718366305\n",
            "Training accuracy: 0.8665821909918212\n",
            "Validation accuracy: 0.8631462538014929\n",
            "--------------------------------------------------------------\n",
            "Epoch: 10\n",
            "Training loss: 0.26849849048457314\n",
            "Validation loss: 0.2768384982783344\n",
            "Training accuracy: 0.8852897131666859\n",
            "Validation accuracy: 0.8811169477467514\n",
            "--------------------------------------------------------------\n",
            "Epoch: 20\n",
            "Training loss: 0.26776829863042895\n",
            "Validation loss: 0.27771883141864934\n",
            "Training accuracy: 0.8847598202972008\n",
            "Validation accuracy: 0.8803796885079717\n",
            "--------------------------------------------------------------\n",
            "Epoch: 30\n",
            "Training loss: 0.2670395643824336\n",
            "Validation loss: 0.2786133313503773\n",
            "Training accuracy: 0.8851284414237991\n",
            "Validation accuracy: 0.8797345866740393\n",
            "--------------------------------------------------------------\n",
            "Epoch: 40\n",
            "Training loss: 0.2661082000289903\n",
            "Validation loss: 0.27824493859460975\n",
            "Training accuracy: 0.8853127519870982\n",
            "Validation accuracy: 0.880840475532209\n",
            "--------------------------------------------------------------\n",
            "Epoch: 49\n",
            "Training loss: 0.26617887193138945\n",
            "Validation loss: 0.2784934578363423\n",
            "Training accuracy: 0.8846215873747264\n",
            "Validation accuracy: 0.8809326329370565\n",
            "--------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXjHD_GFO45n",
        "colab_type": "text"
      },
      "source": [
        "### Plot the training result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O03G7kxO8jv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "fc88ad81-8f55-4f56-838e-122846cf19f0"
      },
      "source": [
        "# Loss curve\n",
        "plt.plot(LR.train_loss)\n",
        "plt.plot(LR.val_loss)\n",
        "plt.title('Loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()\n",
        "\n",
        "# Accuracy curve\n",
        "plt.plot(LR.train_acc)\n",
        "plt.plot(LR.val_acc)\n",
        "plt.title('Accuracy')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WdnLys7-BBF",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model with whole training set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAWSnNfX-FIp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "314eb5f1-0183-4d20-b8a9-0548bdc99784"
      },
      "source": [
        "LR.train(X_train_set, Y_train_set, batch_size = 20, regularization = Regularization.L2, verbose = True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Training loss: 0.36078748584618625\n",
            "Training accuracy: 0.8666513074530584\n",
            "--------------------------------------------------------------\n",
            "Epoch: 10\n",
            "Training loss: 0.2691723438305561\n",
            "Training accuracy: 0.8850362861421495\n",
            "--------------------------------------------------------------\n",
            "Epoch: 20\n",
            "Training loss: 0.26816673657524226\n",
            "Training accuracy: 0.8849671696809124\n",
            "--------------------------------------------------------------\n",
            "Epoch: 30\n",
            "Training loss: 0.26669481541489864\n",
            "Training accuracy: 0.8847367814767884\n",
            "--------------------------------------------------------------\n",
            "Epoch: 40\n",
            "Training loss: 0.26627135701099547\n",
            "Training accuracy: 0.8851975578850363\n",
            "--------------------------------------------------------------\n",
            "Epoch: 49\n",
            "Training loss: 0.26535623874254477\n",
            "Training accuracy: 0.8852666743462735\n",
            "--------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywAWQ55cEp7M",
        "colab_type": "text"
      },
      "source": [
        "### Look at the most significant 10 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUl9op_HE0CW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "b341cdab-4c44-4487-85f7-c04ce625b3f4"
      },
      "source": [
        "# Print out the most significant weights\n",
        "ind = np.argsort(np.abs(LR.W), axis=0)[::-1]\n",
        "with open(X_test_fpath) as f:\n",
        "    content = f.readline().strip('\\n').split(',')\n",
        "features = np.array(content)\n",
        "for i in ind[0:10,0]:\n",
        "    print(f\"{features[i+1].strip():>45} --> {LR.W[i, 0]}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                capital gains --> 1.092774252463708\n",
            "                         weeks worked in year --> 0.7745030573413763\n",
            "                        dividends from stocks --> 0.6813480045725747\n",
            "                                          age --> 0.5828953173016217\n",
            "                                     Nonfiler --> -0.4633655391923445\n",
            "              num persons worked for employer --> 0.3501750904712272\n",
            "       Masters degree(MA MS MEng MEd MSW MBA) --> 0.283113743567795\n",
            "                Other relative of householder --> -0.2822649772019038\n",
            "                                    Nicaragua --> -0.26363891833390335\n",
            "                                         Male --> 0.26354160472501503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss5beuNtRC8l",
        "colab_type": "text"
      },
      "source": [
        "### Compare with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5VXPjkVRGUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "b8d9d474-ceae-4150-984f-05aaff0aaf58"
      },
      "source": [
        "LR_skl = linear_model.LogisticRegression(max_iter=1000, penalty='l2', C = 1e3, n_jobs=-1)\n",
        "LR_skl.fit(X_train, Y_train.reshape(Y_train.shape[0], ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1000.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jokcz_TUSTNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e4a94b6d-1c35-40c1-cbdf-11772c2c5ce6"
      },
      "source": [
        "# Avoid large data copy\n",
        "k = 10\n",
        "subsize = ceil(Y_train.shape[0] / k)\n",
        "Y_train_pred_class = np.zeros_like(Y_train)\n",
        "for i in range(k):\n",
        "    Y_train_pred_class[i*subsize : (i+1)*subsize, 0] = LR_skl.predict(X_train[i*subsize : (i+1)*subsize, :])\n",
        "print(f\"Accuracy of sklearn Logistic Regression: {accuracy(Y_train_pred_class, Y_train)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of sklearn Logistic Regression: 0.8856531996461221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpnFB_8wItmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = permutation_importance(LR_skl, X_train, Y_train,\n",
        "                           n_repeats=30,\n",
        "                           n_jobs=-1,\n",
        "                           random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD34UD9yL3qa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "7dc22276-8063-4d5c-fe68-e3d67545bfad"
      },
      "source": [
        "with open(X_test_fpath) as f:\n",
        "    content = f.readline().strip('\\n').split(',')\n",
        "features = np.array(content)[1:]\n",
        "\n",
        "ind = r.importances_mean.argsort()[::-1]\n",
        "\n",
        "for i in ind[0:10]:\n",
        "    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
        "        print(f\"{features[i]:<50}\"\n",
        "              f\"{r.importances_mean[i]:.3f}\"\n",
        "              f\" +/- {r.importances_std[i]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Spouse of householder                            0.028 +/- 0.001\n",
            " Householder                                      0.024 +/- 0.001\n",
            "weeks worked in year                              0.019 +/- 0.001\n",
            " Spouse of householder                            0.015 +/- 0.001\n",
            "capital gains                                     0.013 +/- 0.000\n",
            " United-States                                    0.011 +/- 0.000\n",
            " Native- Born in the United States                0.010 +/- 0.001\n",
            "age                                               0.010 +/- 0.001\n",
            " Masters degree(MA MS MEng MEd MSW MBA)           0.009 +/- 0.000\n",
            " Both parents present                             0.009 +/- 0.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvTFs1KGkcwC",
        "colab_type": "text"
      },
      "source": [
        "## Predict testing data\n",
        "Now we predict the labels for the testing data and write it to output_logistic.csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "764CYS6okrXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict testing labels\n",
        "predictions = LR.classify(X_test)\n",
        "with open(output_fpath.format('logistic'), 'w') as f:\n",
        "    f.write('id,label\\n')\n",
        "    for i, label in  enumerate(predictions):\n",
        "        f.write('{},{}\\n'.format(i, label[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIjGv4xbY6tH",
        "colab_type": "text"
      },
      "source": [
        "The testing result in https://www.kaggle.com/c/ml2020spring-hw2/data shows the accuracy = 0.88929"
      ]
    }
  ]
}