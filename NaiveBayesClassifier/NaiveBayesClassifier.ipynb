{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayesClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMB6/apJwwZYfr0E15c0qPI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiemanBall/Machine-Learning/blob/master/NaiveBayesClassifier/NaiveBayesClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRgb0mOcBF_w",
        "colab_type": "text"
      },
      "source": [
        "# Classification with Naive Bayes\n",
        "This notebook will build \n",
        "1. a Gaussian Naive Bayes Classifier from scratch with Iris dataset, and\n",
        "2. a Multinomial Naive Bayes Classifier from scratch to classify spam email."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A97i4zc8t9VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import ceil\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, DefaultDict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from scipy.stats import norm\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vzinzsn1siC",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes For Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0yELfJZ_9uQ",
        "colab_type": "text"
      },
      "source": [
        "In binary classification, given the data feature $x \\in R^m $, we want to classify its class. That is,\n",
        "\n",
        "\\begin{equation}\n",
        "P(C_1|x)>0.5?\n",
        "\\end{equation}\n",
        "\n",
        "By Bayes Theorem,\n",
        "\n",
        "\\begin{equation}\n",
        "P(C_1|x) = \\frac{P(x|C_1)P(C_1)}{P(x)}\\propto P(x|C_1)P(C_1)\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "where $P(x)$ is a normalization term, $P(C_1)$ can be treated as prior or approximated by sample distribution. The difficult question is how to get $P(x|C_1)$, and it is where the naive assumptions come in to form a Naive Bayes Classifier.\n",
        "\n",
        "Originally, we have assumptions:\n",
        "1. Label classes are independent,\n",
        "2. Samples are i.i.d.,\n",
        "\n",
        "and now we add one more naive assumption:\n",
        "3. Feature values are independent given the label (conditionally independent)\n",
        "\n",
        "In this case, we can rewrite $P(x|C_1)$ as\n",
        "\n",
        "\\begin{equation}\n",
        "P(x|C_1)=\\prod^m_k{P(x_k|C_1)}\n",
        "\\end{equation}\n",
        "\n",
        "where $x_k$ is the $k$-th feature value in the sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LyVdtw_BtPK",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJWRM_4QBrT6",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing data\n",
        "- Read data\n",
        "- Split to training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CUtZmtXwNF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "Y = iris.target"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_p1y9NPyZEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a954cd9e-d9fb-4a42-adec-8751f4504f81"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 23, test_size = 0.4)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 4)\n",
            "(60, 4)\n",
            "(90,)\n",
            "(60,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU8cb8A31mnH",
        "colab_type": "text"
      },
      "source": [
        "### Build A Gaussian Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TZnHsZP_8cZ",
        "colab_type": "text"
      },
      "source": [
        "A generative model usually predefines the underlying distribution. Here we assume the probabilistic distribution of $P(x_k|C_1)$ is Gaussian with mean $\\mu_k$ and standard deviation $\\sigma_k$.\n",
        "\n",
        "Given a dataset $X\\in R^{n\\times m}$, we want to find the optimal $\\mu_k$ and $\\sigma_k$ to explain the sample distribution the best. That is, maximum likelihood estiamte (MLE)\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{maximize}\\ \\text{likelihood} = P(C_1|X \\in \\{x | C_x = C_1\\})\n",
        "\\end{equation}\n",
        "\\\n",
        "by taking log and combining with naive bayes assumption, this is equivalent to\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{maximize}\\ \\sum^m_k{\\log{P(X_k|C_1)}} \\\\\n",
        "= \\sum^m_k{\\text{maximize}_{\\mu_k, \\sigma_k}\\ log{N(X_k|\\mu_k, \\sigma_k)}}\n",
        "\\end{equation}\n",
        "\\\n",
        "where $N(X_k|\\mu_k, \\sigma_k)$ is the gaussian distribution of $k$-th feature in the dataset. With the assumption of i.i.d. samples,\n",
        "\n",
        "\\begin{equation}\n",
        "\\log{N(X_k|\\mu_k, \\sigma_k)} = \\sum^n_i{\\log{N(x^i_k|\\mu_k, \\sigma_k)}}\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "We will skip the derivation of computing the optimal $\\mu_k$ and $\\sigma_k$ here, but the result is using the sample mean and sample standard deviation, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8XjxxqLzjtV",
        "colab_type": "text"
      },
      "source": [
        "To avoid underflow, we compute the log probability \n",
        "\n",
        "\\begin{equation}\n",
        "\\log{(P(x|C_1)P(C_1))}\n",
        "= \\sum^m_k{\\{\\log{N(x_k|\\mu_k, \\sigma_k)}\\}} + \\log{P(C_1)}\\\\\n",
        "\\log{P(x)} \n",
        "= \\log{\\sum^C_c{(P(x|C_c)P(C_c))}} \n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifqu6kjXUmuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GNB:\n",
        "    def __init__(self):\n",
        "        self.means_by_class = None\n",
        "        self.stds_by_class  = None\n",
        "        self.prior_by_class = None\n",
        "        self.unique_classes = None\n",
        "\n",
        "\n",
        "    def get_prior_by_class(self, Y):\n",
        "        '''\n",
        "        Compute the prior of each class based on the sample distribution\n",
        "        Inputs:\n",
        "            Y: (N, ) ndarray. N data labels\n",
        "\n",
        "        Outputs:\n",
        "            prior_by_class: defaultdict(key=int, val=float). The key is the label id, and the value \n",
        "                            is the prior belonging to the label class.\n",
        "        '''\n",
        "        self.unique_classes = sorted(set(Y))\n",
        "        self.prior_by_class = defaultdict(float)\n",
        "        cnt = Counter(Y)\n",
        "        total = sum(cnt.values())\n",
        "        for label in self.unique_classes:\n",
        "            self.prior_by_class[label] = cnt[label] / total\n",
        "\n",
        "\n",
        "    def separate_data_by_class(self, X: np.ndarray, Y: np.ndarray)->DefaultDict:\n",
        "        '''\n",
        "        Separate samples by class and store them to dictionary type variable\n",
        "        Inputs:\n",
        "            X: (N,m) ndarray. N data, m features\n",
        "            Y: (N, ) ndarray. N labels\n",
        "        Outputs:\n",
        "            X_by_class: defaultdict(key=int, val=(N_c, m) ndarray). The key is the label number,\n",
        "                        and the value is the samples belonging to the label class. N_c is the number\n",
        "                        of samples in class c\n",
        "        '''\n",
        "        X_by_class = defaultdict(list)\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "            X_by_class[y].append(x)\n",
        "\n",
        "        for label, data_subset in X_by_class.items():\n",
        "            X_by_class[label] = np.array(data_subset)\n",
        "\n",
        "        return X_by_class\n",
        "\n",
        "\n",
        "    def compute_mean_std_by_class(self, X_by_class: DefaultDict):\n",
        "        '''\n",
        "        Compute the mean and standard deviation of each feature in the data by class\n",
        "        Inputs:\n",
        "            X_by_class: defaultdict(key=int, val=(N_c, m) ndarray). The key is the label number,\n",
        "                        and the value is the samples belonging to the label class. N_c is the number\n",
        "                        of samples in class c\n",
        "        Outputs:\n",
        "            self.means_by_class: defaultdict(key=int, val=(m, ) ndarray). The key is the label number,\n",
        "                                 and the value is the (m, ) array of m means of each feature belonging \n",
        "                                 to the label class.\n",
        "            self.stds_by_class:  defaultdict(key=int, val=(m, ) ndarray). The key is the label number,\n",
        "                                 and the value is the (m, ) array of m standard deviations of each feature \n",
        "                                 belonging to the label class.\n",
        "        '''\n",
        "        self.means_by_class = defaultdict(float)\n",
        "        self.stds_by_class  = defaultdict(float)\n",
        "        for label, data_subset in X_by_class.items():\n",
        "            self.means_by_class[label] = np.mean(data_subset, axis=0)\n",
        "            self.stds_by_class[label]  = np.std(data_subset, axis=0)\n",
        "            if self.means_by_class[label].shape[0] != data_subset.shape[1]:\n",
        "                print(\"Wrong dimension!\")\n",
        "\n",
        "\n",
        "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
        "        '''\n",
        "        Fit the Naive Bayes Model with input feature X and label Y\n",
        "        Inputs:\n",
        "            X: (N,m) ndarray. N data, m features\n",
        "            Y: (N, ) ndarray. N labels\n",
        "        '''\n",
        "        self.get_prior_by_class(Y)\n",
        "\n",
        "        # Separate X by class\n",
        "        X_by_class = self.separate_data_by_class(X, Y)\n",
        "        # Compute the optimal mean and std for each\n",
        "        self.compute_mean_std_by_class(X_by_class)\n",
        "\n",
        "\n",
        "    def log1DGaussian(self, X, mean=0.0, std=1.0):\n",
        "        '''\n",
        "        Compute the log of normal distribution of x. Support single feature or one sample point\n",
        "        Inputs:\n",
        "            X: (N, m) ndarray. N = number of data, m = number of features\n",
        "            mean: (m, ) ndarray. m = number of features\n",
        "            std: (m, ) ndarray. m = number of features\n",
        "\n",
        "        Outputs:\n",
        "            log of normal distribution: (N, m) ndarray\n",
        "        '''\n",
        "        return -np.square((X - mean) / std) / 2 - np.log(np.sqrt(2 * np.pi) * std)\n",
        "\n",
        "\n",
        "    def compute_all_jll(self, X):\n",
        "        '''\n",
        "        Compute joint log likelihood of X for all classes\n",
        "        Input:\n",
        "            X: (N, m) ndarray. N = number of data, m = number of features\n",
        "\n",
        "        Output:\n",
        "            jll_all: (N, C) ndarray. N = number of data, C = number of classes\n",
        "        '''\n",
        "        jll_all = []\n",
        "\n",
        "        for label in self.unique_classes:\n",
        "            log_prior = np.log(self.prior_by_class[label])\n",
        "            means = self.means_by_class[label]  # shape=(m, )\n",
        "            stds  = self.stds_by_class[label]   # shape=(m, )\n",
        "            log_cond_prob = np.sum(self.log1DGaussian(X, means, stds), axis = 1) # shape=(N, )\n",
        "            jll_all.append(log_cond_prob + log_prior)\n",
        "            \n",
        "        jll_all = np.array(jll_all).T\n",
        "\n",
        "        return jll_all\n",
        "\n",
        "\n",
        "    def pred_log_prob(self, X):\n",
        "        '''\n",
        "        Compute log probability of X for all classes\n",
        "        Input:\n",
        "            X: (N, m) ndarray. N = number of data, m = number of features\n",
        "\n",
        "        Output:\n",
        "            log_prob: (N, C) ndarray. N = number of data, C = number of classes\n",
        "        '''\n",
        "\n",
        "        num_data, num_features = X.shape\n",
        "        num_classes = len(self.unique_classes)\n",
        "\n",
        "        jll_all = self.compute_all_jll(X)  # shape = (num_data, num_classes)\n",
        "\n",
        "        log_p_x = logsumexp(jll_all, axis = 1).reshape(-1, 1) # shape = (num_data, 1)\n",
        "\n",
        "        return jll_all - log_p_x\n",
        "\n",
        "\n",
        "    def pred_prob(self, X: np.ndarray)->np.ndarray:\n",
        "        '''\n",
        "        Compute probability of X for all classes\n",
        "        Input:\n",
        "            X: (N, m) ndarray. N = number of data, m = number of features\n",
        "\n",
        "        Output:\n",
        "            prob: (N, C) ndarray. N = number of data, C = number of classes\n",
        "        '''\n",
        "        return np.exp(self.pred_log_prob(X))\n",
        "\n",
        "\n",
        "    def predict(self, X: np.ndarray)->Tuple[np.ndarray, np.ndarray]:\n",
        "        '''\n",
        "        Predict each class of input X.\n",
        "        Inputs:\n",
        "            X: (N, m) ndarray.\n",
        "        Outputs:\n",
        "            pred_classes: (N, ) ndarray. N = number of data\n",
        "            pred_probs: (N, C) ndarray. N = number of data, C = number of classes\n",
        "        '''\n",
        "        pred_probs   = self.pred_prob(X)\n",
        "        pred_classes = np.argmax(pred_probs, axis = 1)\n",
        "\n",
        "        return pred_classes, pred_probs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkRbNm0cPtLk",
        "colab_type": "text"
      },
      "source": [
        "### Train GNB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvn-XFBlMOfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gnb = GNB()\n",
        "gnb.fit(X_train, Y_train)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztccl3heXxQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "2daa6e8d-08cd-4dce-bc59-562a2b91d530"
      },
      "source": [
        "for label in gnb.unique_classes:\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"mean: {gnb.means_by_class[label]}\")\n",
        "    print(f\"std: {gnb.stds_by_class[label]}\")\n",
        "    print(f\"prior: {gnb.prior_by_class[label]}\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n",
            "mean: [4.95862069 3.37931034 1.4137931  0.24827586]\n",
            "std: [0.37279674 0.39599661 0.15914456 0.11925253]\n",
            "prior: 0.32222222222222224\n",
            "\n",
            "Label: 1\n",
            "mean: [5.921875 2.746875 4.26875  1.3375  ]\n",
            "std: [0.48652748 0.32690248 0.48633675 0.20425168]\n",
            "prior: 0.35555555555555557\n",
            "\n",
            "Label: 2\n",
            "mean: [6.5137931  2.93103448 5.50689655 2.03448276]\n",
            "std: [0.5322199  0.27556018 0.51053119 0.26428284]\n",
            "prior: 0.32222222222222224\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUO2lUPmYW6F",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction on Training and Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVHLNSdnYUj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_classes_train, pred_probs_train = gnb.predict(X_train)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sy_q1zGcLcj",
        "colab_type": "text"
      },
      "source": [
        "#### Compare with Scikit Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ-2163zcPl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "94904382-ec4a-4395-a39d-2c13c5fc3daf"
      },
      "source": [
        "gnb_skl = GaussianNB()\n",
        "gnb_skl.fit(X_train, Y_train)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6zxyo_ccqjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "8d1dc32c-a017-453a-8127-788800422b0d"
      },
      "source": [
        "for label, (mean, var, prior) in enumerate(zip(gnb_skl.theta_, gnb_skl.sigma_, gnb_skl.class_prior_)):\n",
        "    print(f\"Label: {label}\")\n",
        "    print(f\"mean: {mean}\")\n",
        "    print(f\"std: {np.sqrt(var)}\")\n",
        "    print(f\"prior: {prior}\\n\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n",
            "mean: [4.95862069 3.37931034 1.4137931  0.24827586]\n",
            "std: [0.37279674 0.39599662 0.15914457 0.11925254]\n",
            "prior: 0.32222222222222224\n",
            "\n",
            "Label: 1\n",
            "mean: [5.921875 2.746875 4.26875  1.3375  ]\n",
            "std: [0.48652748 0.32690249 0.48633676 0.20425169]\n",
            "prior: 0.35555555555555557\n",
            "\n",
            "Label: 2\n",
            "mean: [6.5137931  2.93103448 5.50689655 2.03448276]\n",
            "std: [0.53221991 0.27556019 0.51053119 0.26428285]\n",
            "prior: 0.32222222222222224\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhPVL9iHg7Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_classes_train_skl = gnb_skl.predict(np.atleast_2d(X_train))\n",
        "pred_probs_train_skl = gnb_skl.predict_proba(np.atleast_2d(X_train))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU2yeeB1dHVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "aaa2475b-5dfe-47c6-cf8e-85583481c42b"
      },
      "source": [
        "print(f\"Differece of predicted labels: {pred_classes_train_skl - pred_classes_train}\")\n",
        "print(f\"Differece of predicted probabilities: \\n{np.sum(pred_probs_train_skl - pred_probs_train, axis = 1)}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Differece of predicted labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Differece of predicted probabilities: \n",
            "[ 4.16333634e-17  7.23872378e-22 -1.71727460e-16  3.38846320e-17\n",
            "  7.49285345e-17  2.45202486e-16  2.63677968e-16 -1.08994929e-16\n",
            " -3.73456826e-17  1.66533454e-16  8.94037814e-25  1.38172392e-19\n",
            " -3.11199759e-16  2.11433905e-22  3.34673901e-22  2.15641036e-16\n",
            " -1.38777878e-16 -9.88385805e-17  6.94892397e-23  1.62413485e-16\n",
            "  1.53089347e-16 -1.11455983e-16  3.27650343e-17 -3.06401475e-16\n",
            " -2.19442520e-16  6.00617854e-24  3.14418630e-18  3.46944695e-18\n",
            " -3.88578059e-16 -1.01728233e-16  1.88770151e-22  7.80625564e-17\n",
            "  6.93889390e-17 -1.79896245e-16 -1.38777878e-16  1.66533454e-16\n",
            " -1.62774745e-16 -1.38777878e-16 -2.43300050e-16  5.42800073e-17\n",
            "  8.94405550e-24 -1.69077517e-16  2.55575516e-17  3.34309006e-25\n",
            "  3.15293152e-16 -5.55111512e-17  3.20710100e-23 -1.83880688e-16\n",
            "  2.03287907e-18 -4.64038530e-17  1.63253292e-23  3.41202157e-25\n",
            " -5.34966362e-16  5.63863125e-25  4.99600361e-16  5.52672057e-17\n",
            "  2.20155432e-23 -5.48606299e-17  1.21647484e-16 -1.77809156e-16\n",
            "  5.99307423e-21  1.67369638e-17 -1.19695920e-16  1.11022302e-16\n",
            "  4.55364912e-17 -1.38777878e-16  7.28029627e-22 -1.91627602e-17\n",
            " -1.17093835e-17 -2.38524478e-17 -1.31838984e-16  8.88439569e-23\n",
            "  4.35756100e-17  6.46639213e-16  3.98334495e-16  1.21254196e-16\n",
            "  3.34044409e-21  8.47443372e-23  6.56527409e-21 -1.43507498e-16\n",
            "  6.74602014e-23  1.75640978e-23 -8.32667268e-17  1.09484931e-22\n",
            " -3.40818953e-16  2.17575903e-22 -1.57859836e-16  1.82679797e-19\n",
            "  9.29429400e-23 -3.88578059e-16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT8fnsvgd3A3",
        "colab_type": "text"
      },
      "source": [
        "### Predict on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9JohHvEd77D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_classes_test, pred_probs_test = gnb.predict(X_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51mtr0GbeGu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "165d5440-1d44-4f80-be8e-9d512041c929"
      },
      "source": [
        "label_names = ['Setosa', 'Versicolor', 'Virginica']\n",
        "print(classification_report(Y_test, pred_classes_test, target_names = label_names))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Setosa       1.00      1.00      1.00        21\n",
            "  Versicolor       0.85      0.94      0.89        18\n",
            "   Virginica       0.95      0.86      0.90        21\n",
            "\n",
            "    accuracy                           0.93        60\n",
            "   macro avg       0.93      0.93      0.93        60\n",
            "weighted avg       0.94      0.93      0.93        60\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}